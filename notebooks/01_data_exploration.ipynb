{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Data Exploration\n\nThis notebook explores the customer data to understand its characteristics and prepare for analysis.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport sys\n\n# Add src directory to path\nsys.path.append(os.path.join(os.path.dirname(os.path.abspath('')), '../src'))\n\n# Import modules\nfrom src.data_preprocessing import DataPreprocessor\n\n# Set visualization style\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette('viridis')\n\n# Display settings\npd.set_option('display.max_columns', 50)\npd.set_option('display.width', 1000)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 1. Load Data",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Initialize data preprocessor\npreprocessor = DataPreprocessor()\n\n# Load data\ndf = preprocessor.load_data('../data/raw/customer_data.csv')\n\n# Display first few rows\ndf.head()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 2. Data Overview",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Get basic information about the dataset\nprint(f\"Dataset shape: {df.shape}\")\nprint(\"\\nData types:\")\nprint(df.dtypes)\nprint(\"\\nMissing values:\")\nprint(df.isnull().sum())\nprint(\"\\nSummary statistics:\")\ndf.describe()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 3. Target Variable Analysis",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Analyze churn distribution\nchurn_counts = df['Churn'].value_counts()\nchurn_percentage = df['Churn'].value_counts(normalize=True) * 100\n\nprint(\"Churn Distribution:\")\nprint(churn_counts)\nprint(\"\\nChurn Percentage:\")\nprint(churn_percentage)\n\n# Visualize churn distribution\nplt.figure(figsize=(8, 6))\nsns.countplot(x='Churn', data=df)\nplt.title('Churn Distribution')\nplt.xlabel('Churn (0=No, 1=Yes)')\nplt.ylabel('Count')\n\n# Add percentage labels\ntotal = len(df)\nfor p in plt.gca().patches:\n    height = p.get_height()\n    plt.gca().text(p.get_x() + p.get_width()/2., height + 50,\n                    f'{height/total*100:.1f}%',\n                    ha='center', va='bottom')\n\nplt.tight_layout()\nplt.savefig('../visualizations/churn_distribution.png', dpi=300)\nplt.show()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 4. Feature Analysis",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Analyze numerical features\nnumerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\nnumerical_features.remove('Churn')\nnumerical_features.remove('CustomerID')\n\nprint(f\"Numerical features: {numerical_features}\")\n\n# Create histograms for numerical features\nplt.figure(figsize=(15, 12))\nfor i, feature in enumerate(numerical_features):\n    plt.subplot(3, 3, i+1)\n    sns.histplot(df[feature], kde=True)\n    plt.title(f'Distribution of {feature}')\n    plt.tight_layout()\n\nplt.savefig('../visualizations/feature_distributions.png', dpi=300)\nplt.show()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Analyze categorical features\ncategorical_features = df.select_dtypes(include=['object']).columns.tolist()\n\nprint(f\"Categorical features: {categorical_features}\")\n\n# Create count plots for categorical features\nif categorical_features:\n    plt.figure(figsize=(15, 10))\n    for i, feature in enumerate(categorical_features):\n        plt.subplot(2, 2, i+1)\n        sns.countplot(y=feature, data=df)\n        plt.title(f'Count of {feature}')\n        plt.tight_layout()\n\n    plt.savefig('../visualizations/categorical_features.png', dpi=300)\n    plt.show()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 5. Correlation Analysis",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Calculate correlation matrix\ncorrelation_matrix = df.corr()\n\n# Plot correlation heatmap\nplt.figure(figsize=(12, 10))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\nplt.title('Correlation Matrix')\nplt.tight_layout()\nplt.savefig('../visualizations/correlation_matrix.png', dpi=300)\nplt.show()\n\n# Show correlations with churn\nchurn_correlations = correlation_matrix['Churn'].sort_values(ascending=False)\nprint(\"Correlations with Churn:\")\nprint(churn_correlations)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 6. Churn by Features",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Analyze churn by numerical features\nplt.figure(figsize=(15, 12))\nfor i, feature in enumerate(numerical_features[:6]):\n    plt.subplot(3, 2, i+1)\n    sns.boxplot(x='Churn', y=feature, data=df)\n    plt.title(f'{feature} by Churn')\n    plt.tight_layout()\n\nplt.savefig('../visualizations/churn_by_features.png', dpi=300)\nplt.show()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 7. Data Preprocessing",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Clean data\ncleaned_df = preprocessor.clean_data(df)\n\n# Encode categorical variables\nencoded_df = preprocessor.encode_categorical(cleaned_df)\n\n# Scale features\nscaled_df = preprocessor.scale_features(encoded_df)\n\n# Save processed data\npreprocessor.save_processed_data(scaled_df, '../data/processed/processed_data.csv')\n\nprint(\"Data preprocessing completed.\")\nprint(f\"Processed data shape: {scaled_df.shape}\")\nprint(\"\\nFirst few rows of processed data:\")\nscaled_df.head()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 8. Summary\n\nThis notebook explored the customer data and performed initial preprocessing. Key findings:\n\n1. The dataset contains 10,000 customers with various features\n2. Churn rate is approximately 20%\n3. Some features show correlation with churn\n4. Data has been cleaned, encoded, and scaled for modeling\n\nNext steps:\n- Customer segmentation using clustering\n- Churn prediction using machine learning models\n- Business insights and recommendations",
      "metadata": {}
    }
  ]
}